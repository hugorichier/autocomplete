{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ws_hugging_face.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoGqG0gbY24X6J0a37SEZR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugorichier/autocomplete/blob/main/autocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIBwzK9FRNgk",
        "outputId": "862419aa-9ed7-4e69-c51e-35c52523aa04"
      },
      "source": [
        "!pip install pytorch_transformers -q\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 184kB 22.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 40.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 55.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 51.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 13.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.7MB 50.7MB/s \n",
            "\u001b[31mERROR: botocore 1.20.101 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trXmZjpODqiJ"
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc19e_-8Fw1S",
        "outputId": "d1bd23a7-dafb-41c1-b38c-a7e9fe00c3e1"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "#text sur lequel on va effectuer des prédictions\n",
        "text = input()\n",
        "predicted_text = text\n",
        "\n",
        "#changer le paramètre de range en fonction du nombre de mots à prédire\n",
        "for n in range(3):\n",
        "  text = predicted_text\n",
        "  indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  #on utilise le modèle gpt2 (transformer)\n",
        "  model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "  #evaluation du modèle\n",
        "  model.eval()\n",
        "\n",
        "  #on désactive les calculs de gradients pour aller plus vite\n",
        "  with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "\n",
        "  predicted_index = torch.argmax(predictions[0, len(predicted_text.split())-1, :]).item()\n",
        "  predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptkl8L7DF4Xb",
        "outputId": "ae9f3556-3140-44f0-a459-f0e9b3f34806"
      },
      "source": [
        "#afficher la prédiction\n",
        "print(predicted_text)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " How to Use the\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}